# -*- coding: utf-8 -*-
"""Travel Times to Flows

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z2olx7TSzpF3WgMy_F1m8avo-JTRcrOX
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import os 
import matplotlib.pyplot as plt

!pip install -q sklearn

from sklearn.model_selection import train_test_split

# %tensorflow_version 2.x  # this line is not required unless you are in a notebook
import tensorflow as tf  # now import the tensorflow module
print(tf.version)  # make sure the version is 2.x
from tensorflow import keras

path = "/content/drive/MyDrive/Colab Notebooks/Training and Testing Data June-5.csv"
Main_DF = pd.read_csv(path)
Main_DF.shape
Main_DF['Gamma'] = Main_DF['Intersection Count']
Main_DF['Alpha+Beta'] = Main_DF['Destination_Count']-Main_DF['Intersection Count']

#Main_DF['Intersection Count'].hist(bins=100)
print("Distribution of Vehicles entering from Sides (Alpha+Beta) \n")
Main_DF['Alpha+Beta'].hist(bins=100)

#Main_DF.columns
#Main_DF['Intersection Count'].describe()
#print("\n Count of Merged Vehicles with respect to Frequency \n")
Main_DF['Alpha+Beta'].describe()
Main_DF.columns



Main_DF.columns
Main_DF.describe()
Main_DF = Main_DF.rename(columns={"('Duration_Traffic (s)', '0')":"T1","('Duration_Traffic (s)', '1')":"T2","('Duration_Traffic (s)', '2')":"T3"})
Main_DF['T3'].hist(bins=50)

Main_DF.isnull().values.sum() != 0
Main_DF_nan = Main_DF.loc[Main_DF['T3']==np.nan]

Main_DF_Dropped = Main_DF.dropna()
Main_DF_Dropped.shape
Main_DF = Main_DF_Dropped

Main_DF = Main_DF.rename(columns={"('Duration_Traffic (s)', '0')":"T1","('Duration_Traffic (s)', '1')":"T2","('Duration_Traffic (s)', '2')":"T3"})


## Normalizing the Datasets columns wise 
Main_DF=(Main_DF-Main_DF.mean())/Main_DF.std()
#Main_DF = Main_DF[Main_DF['T1','T2','T3'].notnull()]
# Main_DF['T1'],Main_DF['T2'],Main_DF['T3'] = Main_DF['T1']/Main_DF['T1'].max(),Main_DF['T2']/Main_DF['T2'].max(),Main_DF['T3']/Main_DF['T3'].max()

train, test = train_test_split(Main_DF, test_size=0.2)


train_Y,test_Y = train[['Alpha+Beta', 'Gamma']],test[['Alpha+Beta', 'Gamma']]
train_X,test_X = train[["T1","T2","T3"]],test[["T1","T2","T3"]]
train_X
#Main_DF.describe()
Main_DF.shape

print(f"Shape of Training Y: {train_Y.shape}")
print(f"Shape of Training X: {train_X.shape}")
print(f"Shape of Testing Y: {test_Y.shape}")
print(f"Shape of Tetsing X: {test_X.shape}")

train_X = train_X.dropna()
print(f"Shape of Training X: {train_X.shape}")

#train_Y_numpy,test_Y_numpy = train_Y.to_numpy().transpose(), test_Y.to_numpy().transpose()
#train_X_numpy,test_X_numpy = train_X.to_numpy().transpose(),test_X.to_numpy().transpose()

#train_X_numpy.shape

test_X_numpy.shape

train_Y_numpy.shape

model = keras.Sequential([
  # keras.layers.Flatten(input_shape=(2075,3)),  # input layer (1)
  keras.layers.Dense(120, activation='relu'),  # hidden layer (2)
  keras.layers.Dense(60,activation='relu'),   # hidden layer (3)
  keras.layers.Dense(30,activation='relu'), # hidden layer (4)
  keras.layers.Dense(2) # output layer (4)
])

opt = keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=opt,
              loss='mean_absolute_error',
              metrics=['accuracy'])

history = model.fit(train_X, train_Y, epochs=10000)

loss = history.history['loss']
accuracy = history.history['accuracy']

fig, ax = plt.subplots(constrained_layout=True)
plt.figure(1)
plt.plot(loss,"r")
plt.plot(accuracy,"b")

ax.set_xlim(0,100)
ax.set_ylim(0,1)
ax.set_title('Cost Function and Accuracy')
ax.set_xlabel("No. of epochs")
ax.set_ylabel("Cost Function")

history = model.fit(train_X, train_Y, epochs=10000, batch_size=5)

loss = history.history['loss']
accuracy = history.history['accuracy']

fig, ax = plt.subplots(constrained_layout=True)
plt.figure(1)
plt.plot(loss,"r")
plt.plot(accuracy,"b")

ax.set_xlim(0,10000)
ax.set_ylim(0,1)
ax.set_title('Cost Function and Accuracy')
ax.set_xlabel("No. of epochs")
ax.set_ylabel("Cost Function")

test_loss, test_acc = model.evaluate(test_X,  test_Y, verbose=1)

train_X

"""# Pytorch"""

import torch
import torch.nn.functional as F


# torch.manual_seed(1)    # reproducible
x = torch.tensor(train_X.values).float()
y = torch.tensor(train_Y.values).float()


class Net(torch.nn.Module):
    def __init__(self, n_feature, n_output):
        super(Net, self).__init__()
        self.hidden_layers = torch.nn.Sequential(
            # Defining a 2D convolution layer
            torch.nn.Linear(n_feature, 120),
            torch.nn.ReLU(inplace=True),
            torch.nn.Linear(120, 60),
            torch.nn.ReLU(inplace=True),
            torch.nn.Linear(60, 30),
            torch.nn.ReLU(inplace=True),
        )

        self.output_layers = torch.nn.Sequential(
            torch.nn.Linear(30, n_output)
        )

    def forward(self, x):
        x = self.hidden_layers(x)      # activation function for hidden layer
        x = self.output_layers(x)             # linear output
        return x

net = Net(n_feature=3, n_output=2)     # define the network
print(net)  # net architecture

optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
loss_func = torch.nn.L1Loss()  # this is for regression mean squared loss

loss_list = []
epoch_iter = []
epochs = 10000
for i in range(epochs):
    prediction = net(x)    
    loss = loss_func(prediction, y)
    loss_list.append(loss)
    optimizer.zero_grad()   # clear gradients for next train
    loss.backward()         # backpropagation, compute gradients
    optimizer.step()        # apply gradients
    print ('Epoch [{}/{}], Loss: {:.4f}' .format(i+1, epochs, loss.item()))





